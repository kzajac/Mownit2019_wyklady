\section{12.8 Metoda kolejnych nadrelaksacji - SOR (successive over-relaxation)}

\begin{frame}{Metoda kolejnych nadrelaksacji - SOR (successive over-relaxation)}
 Na podstawie metody successive relaxation (czyli Gaussa-Seidla) 
można zapisać :
$$x^{(t+1)}_{i}= x^{(t)}_{i} +\omega \underbrace{\frac{1}{a_{ii}}[b_i-\sum^{i-1}_{j=1} a_{ij} x^{(t+1)}_j -\sum^{n}_{j=i} a_{ij} x^{(t)}_j ]}_{r^{(t)}_i \text{- poprawka do starego rozwiązania } x^{(t)}_i}$$
  %stare roziwązanie? poprawione z śtaregożozwiazania
  Dla metody GS $\omega=1$\\
  Przyspieszenie zbieżności:
  $$\boxed{x^{(t+1)}_{i}=x^{(t)}_{i}+\omega r^{(t)}_{i}}, \text{$\omega$ - pewna liczba}$$
\end{frame}

\begin{frame}
Można też zapisać:
 $$x^{(t+1)}_{i}= (1-\omega)x^{(t)}_{i} +\omega \frac{1}{a_{ii}}[b_i-\sum^{i-1}_{j=1} a_{ij} x^{(t+1)}_j -\sum^{n}_{j=i+1} a_{ij} x^{(t)}_j ] $$
  %$$a_{ii}x^{(t+1)}_{i}=\underbrace{a_{ii}}_{\Delta}x^{(t)}_{i}+\omega[b_i-\sum^{i-1}_{j=1}a_{ij}x^{(t+1)}_{j}-\sum^{n}_{j=i+1}a_{ij}x^{(t)}_{j}]-\omega\underbrace{a_{ii}}_{\Delta}x^{(t)}_{i}$$
  W zapisie macierzowym
  $$Dx^{(t+1)}=(1-\omega )Dx^{(t)}+\omega [b-Lx^{(t+1)}-Ux^{(t)}]$$
  po uporządkowaniu:
  $$x^{(t+1)}=\underbrace{(D+\omega L)^{-1}[D-\omega (D+U)]}_{M}x^{(t)}+\underbrace{\omega(D+\omega L)^{-1}b}_{W(=B^{-1}b)}$$
\end{frame}

\begin{frame}{}
  \textbf{Twierdzenie}
  \begin{block}{Założenia}
    Dla dowolnej nieosobliwej macierzy A i dowolnej liczby $\omega$ zachodzi:
  \end{block}
  \begin{block}{Teza}
    $$\rho(M)\geq |\omega -1|$$
    Stąd:
    $$
    \omega\in(0,2) \Rightarrow
    \begin{cases}
      \omega\in{(0,1]}\text{\quad- podrelaksacja}\\
      \omega\in{(1,2)}\text{\quad- nadrelaksacja}
    \end{cases}
    $$
  \end{block}
  Dla ważnych praktycznie klas macierzy znana jest optymalna wartość $\omega$, która zależy od pochodzenia macierzy (zwykler. różniczkowego)
\end{frame}

%\begin{frame}{}
  
%  Dla ważnych praktycznie klas macierzy znana jest optymalna wartość $\omega$
  %\includegraphics[height=0.8\textheight, width=1\textwidth]{img/12/iteracja2}
%\end{frame}

\begin{frame}{}
  \textbf{Twierdzenie}
  \begin{block}{Założenia}
    Dla A - symetrycznej, dodatnio określonej o postacji blokowo - trójprzekątniowej:
    $$
    A=\begin{bmatrix}
    D_1 & U_1 &&&&\\
    L_2 & D_2 & U_2 &&& \\
    &&...&...&&\\
    &&&L_{n-1}&D_{n-1}&U_{n-1}\\
    &&&&L_n&D_n
    \end{bmatrix}
    $$
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Teza}
    $$\rho(M_{GS})=\rho^2(M_J)$$
    $$\omega_{opt}=\frac{2}{1+\sqrt{1-\rho(M_{GS})}}$$
    $$\lambda_{SOR}=\omega_{opt}-1$$
  \end{block}
  \begin{exampleblock}{Dla równania modelowego}
     $$\rho(M_{GS})=cos^2(\frac{\pi}{N}), \omega_{opt}\approx2(1-\frac{\pi}{N}), \lambda_{SOR}\approx 1-\frac{2\pi}{N}$$
    $$
    R_{SOR}\approx -log_{10}(1-\frac{2\pi}{N})=0.4343 \frac{2\pi}{N}
    $$
    SOR wymaga
    $
    \frac{R_{SOR}}{R_{GS}}=\frac{2N}{\pi}
    $ razy mniej iteracji niż GS
  \end{exampleblock}
\end{frame}


